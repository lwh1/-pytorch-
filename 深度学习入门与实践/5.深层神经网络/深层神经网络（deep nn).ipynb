{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深层神经网络\n",
    "\n",
    "下面我们直接用minst举例，讲一讲深度神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "from torchvision.datasets import mnist  #从pytorch内置的数据集中导入minst\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用内置函数下载mnist数据集\n",
    "train_set = mnist.MNIST('./data', train = True, download = True)\n",
    "test_set = mnist.MNIST('./data', train = False, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我们可以看看其中一个数据是什么样子的\n",
    "a_data, a_label = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x22B226719E8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里读入的是PIL数据库中的格式，我们可以很方便的将它转换成numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "a_data = np.array(a_data, dtype = 'float32')\n",
    "print(a_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到图片的大小是28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3.  18.\n",
      "   18.  18. 126. 136. 175.  26. 166. 255. 247. 127.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.  30.  36.  94. 154. 170. 253.\n",
      "  253. 253. 253. 253. 225. 172. 253. 242. 195.  64.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.  49. 238. 253. 253. 253. 253. 253.\n",
      "  253. 253. 253. 251.  93.  82.  82.  56.  39.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.  18. 219. 253. 253. 253. 253. 253.\n",
      "  198. 182. 247. 241.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.  80. 156. 107. 253. 253. 205.\n",
      "   11.   0.  43. 154.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.  14.   1. 154. 253.  90.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 139. 253. 190.\n",
      "    2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  11. 190. 253.\n",
      "   70.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  35. 241.\n",
      "  225. 160. 108.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  81.\n",
      "  240. 253. 253. 119.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   45. 186. 253. 253. 150.  27.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.  16.  93. 252. 253. 187.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0. 249. 253. 249.  64.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   46. 130. 183. 253. 253. 207.   2.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  39. 148.\n",
      "  229. 253. 253. 253. 250. 182.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  24. 114. 221. 253.\n",
      "  253. 253. 253. 201.  78.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.  23.  66. 213. 253. 253. 253.\n",
      "  253. 198.  81.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  18. 171. 219. 253. 253. 253. 253. 195.\n",
      "   80.   9.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.  55. 172. 226. 253. 253. 253. 253. 244. 133.  11.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0. 136. 253. 253. 253. 212. 135. 132.  16.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print(a_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将数组表示出来，0表示黑色，255表示白色\n",
    "\n",
    "对于神经网络，第一层的输入就是28x28 = 784，所以需要对数据进行一次变化，用reshape将它们拉平为一维矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tf(x):\n",
    "    x = np.array(x, dtype = 'float32') / 255\n",
    "    x = (x - 0.5) / 0.5\n",
    "    x = x.reshape((-1, ))\n",
    "    x = t.from_numpy(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重新载入数据集，声明定义的数据变换\n",
    "\n",
    "train_set = mnist.MNIST('./data', train = True, transform = data_tf, download = True)\n",
    "test_set = mnist.MNIST('./data', train = True, transform = data_tf, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "a, a_label = train_set[0]\n",
    "print(a.shape)\n",
    "print(a_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用pytorch自带的DataLoader定义一个数据迭代器\n",
    "from torch.utils.data import DataLoader\n",
    "train_data = DataLoader(train_set, batch_size = 64, shuffle = True)\n",
    "test_data = DataLoader(test_set, batch_size = 128, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用数据迭代器是很有必要的，如果数据量太大，就无法将它们一次全部读入内存，所以需要使用迭代器，每次传入一部分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, a_label = next(iter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#打印出一个批次数据的大小\n",
    "print(a.shape)\n",
    "print(a_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用Sequential定义一个四层神经网络\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(784, 400), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(400, 200), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵在pytorch中已经内置了，交叉熵的数值稳定性更差，内置函数已经帮我们解决了这个问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义loss函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.SGD(net.parameters(), lr = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始训练\n",
    "\n",
    "losses = []\n",
    "acces = []\n",
    "eval_losses = []\n",
    "eval_acces = []\n",
    "\n",
    "for e in range(20):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    net.train() #模型设置为训练模式\n",
    "    for im,label in train_data:\n",
    "        #前向传播\n",
    "        out = net(im)\n",
    "        loss = criterion(out, label)\n",
    "        \n",
    "        #反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #记录误差\n",
    "        train_loss += loss.data.item()\n",
    "        #计算分类的准确性\n",
    "        _,pred = out.max(1)\n",
    "        num_correct = (pred == label).sum().data.item()\n",
    "        acc = num_correct / im.shape[0]\n",
    "        train_acc += acc\n",
    "        \n",
    "    losses.append(train_loss / len(train_data))\n",
    "    acces.append(train_acc / len(train_data))\n",
    "    \n",
    "    #在测试集上检验效果\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    net.eval() #模型设置为测试模式\n",
    "    for im,label in test_data:\n",
    "        out = net(im)\n",
    "        loss = criterion(out, label)\n",
    "        #记录误差\n",
    "        eval_loss += loss.data.item()\n",
    "        #记录准确率\n",
    "        _, pred = out.max(1)\n",
    "        num_correct = (pred == label).sum().data.item()\n",
    "        acc = num_correct / im.shape[0]\n",
    "        eval_acc += acc\n",
    "        \n",
    "    eval_losses.append(eval_loss / len(test_data))\n",
    "    eval_acces.append(eval_acc / len(test_data))\n",
    "    print('epoch:{}, Train_loss:{:.6f}, Train_acc:{:.6f}, Eval_loss:{:.6f}, Eval_acc:{:.6f}'.format(e, train_loss/ len(train_data), train_acc/ len(train_data), eval_loss / len(test_data), eval_acc / len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画出loss曲线和准确率曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"train_loss\")\n",
    "plt.plot(np.arange(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('train_acc')\n",
    "plt.plot(np.arange(len(acces)), acces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('test_loss')\n",
    "plt.plot(np.arange(len(eval_losses)), eval_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('test_acc')\n",
    "plt.plot(np.arange(len(eval_acces)), eval_acces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到我们的三层网络在训练集上能够达到 97.8% 的准确率，测试集上能够达到 98% 的准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**小练习：看一看上面的训练过程，看一下准确率是怎么计算出来的，特别注意 max 这个函数**\n",
    "\n",
    "**自己重新实现一个新的网络，试试改变隐藏层的数目和激活函数，看看有什么新的结果**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准确率的计算方法，我个人的理解是这样的：out是一个行向量，max（1）实际上是按列取最大值，其实还是out本身，只是将label和数据分离开了，后面再通过预测值pred和label值相等的个数（预测正确）和样本总数的比值来作为预测的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重新实现一个新的网络\n",
    "net_1 = nn.Sequential(\n",
    "    nn.Linear(784, 400),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(400,300),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(300, 200),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(200, 100),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(100, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=400, out_features=300, bias=True)\n",
       "  (3): Tanh()\n",
       "  (4): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (5): Tanh()\n",
       "  (6): Linear(in_features=200, out_features=100, bias=True)\n",
       "  (7): Tanh()\n",
       "  (8): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义loss函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.SGD(net_1.parameters(), lr = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Train Loss: 1.307801, Train Acc: 0.673874, Eval Loss: 0.492519, Eval Acc: 0.868826\n",
      "epoch: 1, Train Loss: 0.382268, Train Acc: 0.893573, Eval Loss: 0.325435, Eval Acc: 0.906844\n",
      "epoch: 2, Train Loss: 0.298539, Train Acc: 0.914062, Eval Loss: 0.265722, Eval Acc: 0.923619\n",
      "epoch: 3, Train Loss: 0.247401, Train Acc: 0.928871, Eval Loss: 0.219947, Eval Acc: 0.937483\n",
      "epoch: 4, Train Loss: 0.206424, Train Acc: 0.939666, Eval Loss: 0.182583, Eval Acc: 0.947517\n",
      "epoch: 5, Train Loss: 0.173956, Train Acc: 0.949544, Eval Loss: 0.153908, Eval Acc: 0.955624\n",
      "epoch: 6, Train Loss: 0.148721, Train Acc: 0.956890, Eval Loss: 0.133055, Eval Acc: 0.961704\n",
      "epoch: 7, Train Loss: 0.130514, Train Acc: 0.961887, Eval Loss: 0.116623, Eval Acc: 0.966318\n",
      "epoch: 8, Train Loss: 0.116745, Train Acc: 0.965902, Eval Loss: 0.103472, Eval Acc: 0.970133\n",
      "epoch: 9, Train Loss: 0.105424, Train Acc: 0.968733, Eval Loss: 0.092620, Eval Acc: 0.973270\n",
      "epoch: 10, Train Loss: 0.095491, Train Acc: 0.971932, Eval Loss: 0.083275, Eval Acc: 0.976002\n",
      "epoch: 11, Train Loss: 0.086400, Train Acc: 0.974913, Eval Loss: 0.075457, Eval Acc: 0.978434\n",
      "epoch: 12, Train Loss: 0.078967, Train Acc: 0.976479, Eval Loss: 0.068364, Eval Acc: 0.980899\n",
      "epoch: 13, Train Loss: 0.073167, Train Acc: 0.978545, Eval Loss: 0.062272, Eval Acc: 0.982554\n",
      "epoch: 14, Train Loss: 0.067566, Train Acc: 0.980011, Eval Loss: 0.056764, Eval Acc: 0.984153\n",
      "epoch: 15, Train Loss: 0.062223, Train Acc: 0.981293, Eval Loss: 0.051842, Eval Acc: 0.985935\n",
      "epoch: 16, Train Loss: 0.056789, Train Acc: 0.983725, Eval Loss: 0.047402, Eval Acc: 0.987151\n",
      "epoch: 17, Train Loss: 0.051871, Train Acc: 0.985291, Eval Loss: 0.043244, Eval Acc: 0.988167\n",
      "epoch: 18, Train Loss: 0.048869, Train Acc: 0.985624, Eval Loss: 0.039567, Eval Acc: 0.989517\n",
      "epoch: 19, Train Loss: 0.045717, Train Acc: 0.986824, Eval Loss: 0.036309, Eval Acc: 0.990633\n"
     ]
    }
   ],
   "source": [
    "#开始训练\n",
    "losses = []\n",
    "acces = []\n",
    "eval_losses = []\n",
    "eval_acces = []\n",
    "\n",
    "for e in range(20):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    net_1.train()\n",
    "    \n",
    "    for im, label in train_data:\n",
    "        #前向传播\n",
    "        out = net_1(im)\n",
    "        loss = criterion(out, label)\n",
    "        #反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #记录误差\n",
    "        train_loss += loss.data.item()\n",
    "        #计算准确率\n",
    "        _,pred = out.max(1)\n",
    "        correct_num = (pred == label).sum().data.item()\n",
    "        acc = correct_num / im.shape[0]\n",
    "        train_acc += acc\n",
    "    losses.append(train_loss / len(train_data))\n",
    "    acces.append(train_acc / len(train_data))\n",
    "    \n",
    "    #在测试集上验证效果\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    net_1.eval()\n",
    "    for im, label in test_data:\n",
    "        out = net_1(im)\n",
    "        loss = criterion(out, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        eval_loss += loss.data.item()\n",
    "        \n",
    "        _, pred = out.max(1)\n",
    "        num_correct = (pred == label).sum().data.item()\n",
    "        acc = num_correct / im.shape[0]\n",
    "        eval_acc += acc\n",
    "    \n",
    "    eval_losses.append(eval_loss / len(test_data))\n",
    "    eval_acces.append(eval_acc / len(test_data))\n",
    "    print('epoch: {}, Train Loss: {:.6f}, Train Acc: {:.6f}, Eval Loss: {:.6f}, Eval Acc: {:.6f}'\n",
    "          .format(e, train_loss / len(train_data), train_acc / len(train_data), \n",
    "                     eval_loss / len(test_data), eval_acc / len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
