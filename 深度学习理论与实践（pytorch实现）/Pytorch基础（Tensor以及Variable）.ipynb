{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建一个numpy的ndarray\n",
    "numpy_tensor = np.random.randn(10, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor与ndarray的相互转换\n",
    "可以用以下两种方式将numpy的ndarray转换为pytorch的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4073,  0.0540, -0.2382, -1.3170,  1.3284,  0.5807,  0.8022, -1.0928,\n",
      "          1.3500, -1.0772, -0.3009,  0.9286, -0.3692, -0.2560, -1.3966, -0.6238,\n",
      "         -1.9842,  2.5670, -0.7263, -0.6330],\n",
      "        [ 1.6549, -2.0768,  0.9102, -0.6007,  0.1109,  1.4269, -0.1061,  1.0758,\n",
      "         -0.8092, -0.1247,  0.1619,  0.5499,  1.1338, -0.5454,  0.4168,  2.0563,\n",
      "         -0.1235,  0.5316,  0.9677, -1.8588],\n",
      "        [-0.0396, -1.1029, -2.9897, -1.2262,  1.1872, -1.2366, -1.3382, -0.1344,\n",
      "          1.5564,  0.5822, -0.4220, -0.8209,  1.1765,  0.6056, -0.7796, -1.3003,\n",
      "         -0.8746,  0.0722,  0.7650, -1.1495],\n",
      "        [-0.2582,  0.3231,  0.8763, -1.0808, -0.0996, -1.2569,  0.1388, -0.4465,\n",
      "          0.0971,  1.2675,  0.7545, -1.5415,  0.7036, -0.4816, -1.0728, -1.5231,\n",
      "         -1.7812, -0.5203, -0.2652, -1.6807],\n",
      "        [ 0.7252,  0.3524, -0.0689,  0.7353,  0.0655,  0.7942, -0.5699,  0.6032,\n",
      "          0.5172, -0.8373, -0.6800,  0.8066, -0.5375, -0.5477, -0.2844,  0.9363,\n",
      "          0.3946, -1.0041,  0.1308, -0.2018],\n",
      "        [-0.6566, -0.3077,  2.2193, -1.1927, -0.5648,  1.5385,  1.1107,  0.3484,\n",
      "          0.6330,  0.4055,  0.1658,  0.2261, -1.0903, -0.3909,  0.2552,  1.6807,\n",
      "         -0.7311, -0.2959, -0.8689, -0.0843],\n",
      "        [-0.0440, -1.3972,  0.3866, -1.9402, -0.7592,  0.5813, -0.7118,  0.4417,\n",
      "          2.1127, -0.0649,  1.1277, -0.8111, -0.3534, -0.3849,  0.2725, -1.0549,\n",
      "         -2.3606, -1.8848,  0.0034,  0.1678],\n",
      "        [ 0.0520, -0.0417, -1.0490,  0.5751, -0.2770,  0.9902,  0.7000,  2.2484,\n",
      "          0.1032,  0.1249, -0.0890, -0.9861, -1.0186,  0.3203, -0.5769,  0.4678,\n",
      "         -2.8104, -1.0769, -1.6194, -0.2230],\n",
      "        [ 0.3003,  0.8315, -2.2443,  1.9019, -0.7371, -0.2941,  1.0180,  0.3562,\n",
      "         -2.0557,  0.0521,  0.2020,  0.6051, -0.5668, -0.9893,  0.0587, -0.0722,\n",
      "         -1.1267,  0.2182, -0.3087,  1.2870],\n",
      "        [-1.0294, -1.0057, -0.7602,  0.5695,  0.3772,  0.6130,  1.6038,  1.0891,\n",
      "         -0.6711,  0.9975, -1.0232, -0.8378, -1.0033, -0.2825,  0.0034, -1.0469,\n",
      "         -0.7617, -0.6352, -0.7720,  0.3604]]) \n",
      " tensor([[-0.4073,  0.0540, -0.2382, -1.3170,  1.3284,  0.5807,  0.8022, -1.0928,\n",
      "          1.3500, -1.0772, -0.3009,  0.9286, -0.3692, -0.2560, -1.3966, -0.6238,\n",
      "         -1.9842,  2.5670, -0.7263, -0.6330],\n",
      "        [ 1.6549, -2.0768,  0.9102, -0.6007,  0.1109,  1.4269, -0.1061,  1.0758,\n",
      "         -0.8092, -0.1247,  0.1619,  0.5499,  1.1338, -0.5454,  0.4168,  2.0563,\n",
      "         -0.1235,  0.5316,  0.9677, -1.8588],\n",
      "        [-0.0396, -1.1029, -2.9897, -1.2262,  1.1872, -1.2366, -1.3382, -0.1344,\n",
      "          1.5564,  0.5822, -0.4220, -0.8209,  1.1765,  0.6056, -0.7796, -1.3003,\n",
      "         -0.8746,  0.0722,  0.7650, -1.1495],\n",
      "        [-0.2582,  0.3231,  0.8763, -1.0808, -0.0996, -1.2569,  0.1388, -0.4465,\n",
      "          0.0971,  1.2675,  0.7545, -1.5415,  0.7036, -0.4816, -1.0728, -1.5231,\n",
      "         -1.7812, -0.5203, -0.2652, -1.6807],\n",
      "        [ 0.7252,  0.3524, -0.0689,  0.7353,  0.0655,  0.7942, -0.5699,  0.6032,\n",
      "          0.5172, -0.8373, -0.6800,  0.8066, -0.5375, -0.5477, -0.2844,  0.9363,\n",
      "          0.3946, -1.0041,  0.1308, -0.2018],\n",
      "        [-0.6566, -0.3077,  2.2193, -1.1927, -0.5648,  1.5385,  1.1107,  0.3484,\n",
      "          0.6330,  0.4055,  0.1658,  0.2261, -1.0903, -0.3909,  0.2552,  1.6807,\n",
      "         -0.7311, -0.2959, -0.8689, -0.0843],\n",
      "        [-0.0440, -1.3972,  0.3866, -1.9402, -0.7592,  0.5813, -0.7118,  0.4417,\n",
      "          2.1127, -0.0649,  1.1277, -0.8111, -0.3534, -0.3849,  0.2725, -1.0549,\n",
      "         -2.3606, -1.8848,  0.0034,  0.1678],\n",
      "        [ 0.0520, -0.0417, -1.0490,  0.5751, -0.2770,  0.9902,  0.7000,  2.2484,\n",
      "          0.1032,  0.1249, -0.0890, -0.9861, -1.0186,  0.3203, -0.5769,  0.4678,\n",
      "         -2.8104, -1.0769, -1.6194, -0.2230],\n",
      "        [ 0.3003,  0.8315, -2.2443,  1.9019, -0.7371, -0.2941,  1.0180,  0.3562,\n",
      "         -2.0557,  0.0521,  0.2020,  0.6051, -0.5668, -0.9893,  0.0587, -0.0722,\n",
      "         -1.1267,  0.2182, -0.3087,  1.2870],\n",
      "        [-1.0294, -1.0057, -0.7602,  0.5695,  0.3772,  0.6130,  1.6038,  1.0891,\n",
      "         -0.6711,  0.9975, -1.0232, -0.8378, -1.0033, -0.2825,  0.0034, -1.0469,\n",
      "         -0.7617, -0.6352, -0.7720,  0.3604]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "pytorch_tensor1 = t.Tensor(numpy_tensor)\n",
    "pytorch_tensor2 = t.from_numpy(numpy_tensor)\n",
    "print(pytorch_tensor1, \"\\n\", pytorch_tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以用以下两种方式将pytorch tensor转换为numpy ndarray.\n",
    "注意如果tensor在GPU上，不能直接转换为ndarray，要先通过.cpu()转换到CPU上后在转换成ndarray，否则会报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor在cpu上\n",
    "numpy_array1 = pytorch_tensor1.numpy()\n",
    "\n",
    "#tensor在GPU上\n",
    "numpy_array2 = pytorch_tensor2.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch GPU加速\n",
    "我们可以使用以下两种方式将tensor放到GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torch\\cuda\\__init__.py:116: UserWarning: \n",
      "    Found GPU0 GeForce GT 750M which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error (10): invalid device ordinal",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c5be4b8a2ac5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#第二种方式更简单，推荐使用\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mgpu_tensor1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#将tensor放到第一个GPU上\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mgpu_tensor2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#将tensor放到第二个GPU上\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error (10): invalid device ordinal"
     ]
    }
   ],
   "source": [
    "#第一种方式是定义CUDA数据类型\n",
    "dtype = t.cuda.FloatTensor  #定义默认GPU数据类型\n",
    "gup_tensor = t.Tensor(10, 20).type(dtype)\n",
    "\n",
    "#第二种方式更简单，推荐使用\n",
    "gpu_tensor1 = t.randn(10, 20).cuda(0) #将tensor放到第一个GPU上\n",
    "gpu_tensor2 = t.randn(10, 20).cuda(1) #将tensor放到第二个GPU上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用第一种方法将tensor放到GPU上时会将数据类型转换成指定的类型，而用第二种方法直接将tensor放到GPU上，类型和之前保持一致。推荐在定义tensor时就指定数据类型，再用第二种方式放到GPU上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而将tensor放回CPU的操作非常简单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpu_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-618456b7e413>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcpu_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpu_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'gpu_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "cpu_tensor = gpu_tensor.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 访问tensor的一些属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20])\n",
      "torch.Size([10, 20])\n"
     ]
    }
   ],
   "source": [
    "#通过以下两种方式获得tensor的大小\n",
    "print(pytorch_tensor1.shape)\n",
    "print(pytorch_tensor1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "#得到tensor的数据类型\n",
    "print(pytorch_tensor1.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#得到tensor的维度\n",
    "print(pytorch_tensor1.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#得到tensor所有元素的个数\n",
    "print(pytorch_tensor1.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "tensor_test = t.randn(3, 2)\n",
    "tensor_test = tensor_test.type(t.DoubleTensor)\n",
    "numpy_test = tensor_test.numpy()\n",
    "print(numpy_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor的操作和numpy非常相似，如果你熟悉numpy的操作，那么tensor基本上是一致的。下面来列举一些操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "x = t.ones(2, 2) #表示产生一个2x2的元素均为1的tensor\n",
    "print(x)\n",
    "print(x.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "#将其转化为整型\n",
    "x = x.long()\n",
    "# x = x.type(t.LongTensor)\n",
    "print(x.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7186,  0.3988, -0.4838,  0.4351],\n",
      "        [-0.5701,  0.4154,  1.4665,  0.1102],\n",
      "        [ 0.1719,  0.4493, -0.6168, -0.5705]])\n"
     ]
    }
   ],
   "source": [
    "x = t.randn(3, 4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4351, 1.4665, 0.4493]) tensor([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "max_value, max_index = t.max(x, dim = 1) #沿着行取最大值，返回最大值以及最大值的索引\n",
    "print(max_value, max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1719, 0.4493, 1.4665, 0.4351]) tensor([2, 2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "max_value, max_index = t.max(x, dim = 0) #沿着列取最大值，返回最大值以及最大值的索引\n",
    "print(max_value, max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3685,  1.4221, -0.5661])\n"
     ]
    }
   ],
   "source": [
    "#沿着行对x求和\n",
    "sum_x_raw = t.sum(x, dim = 1)\n",
    "print(sum_x_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1168,  1.2635,  0.3659, -0.0252])\n"
     ]
    }
   ],
   "source": [
    "#沿着列对x求和\n",
    "sum_x_col = t.sum(x, dim = 0)\n",
    "print(sum_x_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "tensor([[[-0.7186,  0.3988, -0.4838,  0.4351],\n",
      "         [-0.5701,  0.4154,  1.4665,  0.1102],\n",
      "         [ 0.1719,  0.4493, -0.6168, -0.5705]]])\n"
     ]
    }
   ],
   "source": [
    "#增加或减少维度\n",
    "print(x.shape)\n",
    "x_1 = x.unsqueeze(0) #在第一维前增加一个维度\n",
    "print(x_1.shape)\n",
    "print(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "x_2 = x.unsqueeze(1)  #在第二维度增加\n",
    "print(x_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "x_1 = x_1.squeeze(0) #减少第一维\n",
    "print(x_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "x_2 = x_2.squeeze(1) #减少第二维\n",
    "print(x_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = x.unsqueeze(0)\n",
    "print(x.shape)\n",
    "x = x.squeeze() #将tensor中的一维全部去掉\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([4, 3, 5])\n",
      "torch.Size([5, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = t.randn(3, 4, 5)\n",
    "print(x.shape)\n",
    "\n",
    "#使用permute和transpose进行维度转换\n",
    "x = x.permute(1, 0, 2) #permute可以重新排列tensor的维度\n",
    "print(x.shape)\n",
    "\n",
    "x = x.transpose(0, 2) #transpose可以将指定的两个维度交换\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([12, 5])\n",
      "torch.Size([5, 12])\n",
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "#使用view对tensor进行reshape\n",
    "x = t.randn(3, 4, 5)\n",
    "print(x.shape)\n",
    "\n",
    "x = x.view(-1, 5)# -1表示任意的大小，5表示第二维的大小为5\n",
    "print(x.shape)\n",
    "\n",
    "x = x.view(5, -1)# 5 表示第一维的大小为5\n",
    "print(x.shape)\n",
    "\n",
    "x = x.view(3, 20)#重新reshape成（3， 20）的大小\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，pytorch中的大部分操作都支持inplace操作，也就是直接对tensor进行操作而不用开辟新的内存空间，方式非常简单，一般是在操作的符号后面加上_。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "torch.Size([1, 3, 3])\n",
      "torch.Size([3, 1, 3])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x = t.ones(3, 3)\n",
    "print(x.shape)\n",
    "\n",
    "#unsqueeze进行inplace\n",
    "x.unsqueeze_(0)\n",
    "print(x.shape)\n",
    "\n",
    "#transpose进行inplace\n",
    "x.transpose_(1, 0)\n",
    "print(x.shape)\n",
    "\n",
    "#add进行inplace\n",
    "x = t.ones(3, 3)\n",
    "y = t.ones(3, 3)\n",
    "x.add_(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 2., 2., 1.],\n",
      "        [1., 2., 2., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = t.ones(4, 4).type(t.FloatTensor)\n",
    "x[1:3, 1:3] = 2\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### autograd: 自动微分\n",
    "\n",
    "深度学习的算法本质上是通过反向传播求导数，而PyTorch的**`autograd`**模块则实现了此功能。在Tensor上的所有操作，autograd都能为它们自动提供微分，避免了手动计算导数的复杂过程。\n",
    " \n",
    "~~`autograd.Variable`是Autograd中的核心类，它简单封装了Tensor，并支持几乎所有Tensor有的操作。Tensor在被封装为Variable之后，可以调用它的`.backward`实现反向传播，自动计算所有梯度~~ ~~Variable的数据结构如图2-6所示。~~\n",
    "\n",
    "\n",
    "\n",
    "  *从0.4起, Variable 正式合并入Tensor, Variable 本来实现的自动微分功能，Tensor就能支持。读者还是可以使用Variable(tensor), 但是这个操作其实什么都没做。建议读者以后直接使用tensor*. \n",
    "  \n",
    "  要想使得Tensor使用autograd功能，只需要设置`tensor.requries_grad=True`. \n",
    "\n",
    "\n",
    "~~Variable主要包含三个属性。~~\n",
    "~~- `data`：保存Variable所包含的Tensor~~\n",
    "~~- `grad`：保存`data`对应的梯度，`grad`也是个Variable，而不是Tensor，它和`data`的形状一样。~~\n",
    "~~- `grad_fn`：指向一个`Function`对象，这个`Function`用来反向传播计算输入的梯度，具体细节会在下一章讲解。~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里仅对Variable做简单介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-8.6180)\n",
      "<SumBackward0 object at 0x0000020AFF54EB00>\n"
     ]
    }
   ],
   "source": [
    "x_tensor = t.randn(10, 5)\n",
    "y_tensor = t.randn(10, 5)\n",
    "\n",
    "#将tensor转换为Variable\n",
    "x = Variable(x_tensor, requires_grad = True) #Variable默认不需要求梯度，如果需要则将requires_grad设置为True\n",
    "y = Variable(y_tensor, requires_grad = True)\n",
    "z = t.sum(x + y)\n",
    "print(z.data)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#求x和y的梯度\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(t.Tensor([2]), requires_grad = True)\n",
    "y = x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
